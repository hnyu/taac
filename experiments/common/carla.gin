import torch

import alf
import alf.algorithms.merlin_algorithm
import alf.environments.suite_carla

CameraSensor.image_size_x=128
CameraSensor.image_size_y=64
CameraSensor.fov=135

create_environment.env_name='Town01'
create_environment.env_load_fn=@suite_carla.load
create_environment.num_parallel_environments=4

use_batch_normalization=False
BottleneckBlock.with_batch_normalization=%use_batch_normalization
preproc_bn=False

suite_carla.Player.with_gnss_sensor=False

camera_spec = alf.nest.get_field(%observation_spec, "observation.camera")
radar_spec = alf.nest.get_field(%observation_spec, "observation.radar")
collision_spec = alf.nest.get_field(%observation_spec, "observation.collision")
#gnss_spec = alf.nest.get_field(%observation_spec, "observation.gnss")
imu_spec = alf.nest.get_field(%observation_spec, "observation.imu")
goal_spec = alf.nest.get_field(%observation_spec, "observation.goal")
velocity_spec = alf.nest.get_field(%observation_spec, "observation.velocity")
navigation_spec = alf.nest.get_field(%observation_spec, "observation.navigation")
prev_action_spec = alf.nest.get_field(%observation_spec, "prev_action")

observation_preprocessors = {
    "camera": @ResnetEncodingNetwork(input_tensor_spec=%camera_spec, output_size=%encoding_dim, output_activation=alf.math.identity, use_fc_bn=%preproc_bn),
    "radar": torch.nn.Sequential(alf.layers.Reshape([-1]), alf.layers.FC(@flattened_size(%radar_spec), %encoding_dim, use_bias=False, use_bn=%preproc_bn)),
    "collision": torch.nn.Sequential(alf.layers.Reshape([-1]), alf.layers.FC(@flattened_size(%collision_spec), %encoding_dim, use_bias=False, use_bn=%preproc_bn)),
    #"gnss": torch.nn.Sequential(alf.layers.Reshape([-1]), alf.layers.FC(@flattened_size(%gnss_spec), %encoding_dim, use_bias=False, use_bn=%preproc_bn)),
    "imu": torch.nn.Sequential(alf.layers.Reshape([-1]), alf.layers.FC(@flattened_size(%imu_spec), %encoding_dim, use_bias=False, use_bn=%preproc_bn)),
    "goal": torch.nn.Sequential(alf.layers.Reshape([-1]), alf.layers.FC(@flattened_size(%goal_spec), %encoding_dim, use_bias=False, use_bn=%preproc_bn)),
    "velocity": torch.nn.Sequential(alf.layers.Reshape([-1]), alf.layers.FC(@flattened_size(%velocity_spec), %encoding_dim, use_bias=False, use_bn=%preproc_bn)),
    "navigation": torch.nn.Sequential(alf.layers.Reshape([-1]), alf.layers.FC(@flattened_size(%navigation_spec), %encoding_dim, use_bias=False, use_bn=%preproc_bn)),
}

input_preprocessors={
    'observation': %observation_preprocessors,
    'prev_action': torch.nn.Sequential(alf.layers.Reshape([-1]), alf.layers.FC(@flattened_size(%prev_action_spec), %encoding_dim, use_bias=False, use_bn=%preproc_bn)),
}


import alf.environments.alf_wrappers

suite_carla.load.wrappers=[@ActionObservationWrapper, @ScalarRewardWrapper]

suite_carla.Player.sparse_reward=False
suite_carla.Player.allow_negative_distance_reward=True
suite_carla.Player.max_collision_penalty=20.
suite_carla.Player.max_red_light_penalty=20.


import alf.environments.carla_controller

CarlaEnvironment.num_other_vehicles=20
CarlaEnvironment.num_walkers=20
CarlaEnvironment.day_length=1000
CarlaEnvironment.step_time=0.1
CarlaEnvironment.vehicle_filter="vehicle.tesla.model3"

import alf.algorithms.encoding_algorithm

encoding_dim = 256 # used by 'carla.gin'
fc_layers_params = (256,)

actor/NormalProjectionNetwork.state_dependent_std=True
actor/NormalProjectionNetwork.scale_distribution=True
actor/NormalProjectionNetwork.std_transform=@clipped_exp
clipped_exp.clip_value_min=-10
clipped_exp.clip_value_max=2

actor/ActorDistributionNetwork.fc_layer_params=%fc_layers_params
actor/ActorDistributionNetwork.continuous_projection_net_ctor=@actor/NormalProjectionNetwork

critic/CriticNetwork.joint_fc_layer_params=%fc_layers_params

AdamTF.lr=1e-4

# config EncodingAlgorithm
encoder/EncodingNetwork.input_preprocessors=%input_preprocessors
encoder/EncodingNetwork.preprocessing_combiner=@NestSum(activation=@torch.relu_, average=True)
encoder/EncodingNetwork.fc_layer_params=%fc_layers_params
encoder/EncodingAlgorithm.encoder_cls=@encoder/EncodingNetwork

# training config
RewardNormalizer.clip_value=5.0
TrainerConfig.data_transformer_ctor=[@agent/ImageScaleTransformer,
                                     @agent/ObservationNormalizer,
                                     @RewardNormalizer]
agent/ImageScaleTransformer.min=0.0
agent/ImageScaleTransformer.fields=['observation.camera']
agent/ObservationNormalizer.clipping=5.0

TrainerConfig.temporally_independent_train_step=True
TrainerConfig.use_rollout_state=True
TrainerConfig.initial_collect_steps=10000
TrainerConfig.mini_batch_length=2
TrainerConfig.unroll_length=10
TrainerConfig.mini_batch_size=64
TrainerConfig.num_updates_per_train_iter=1
TrainerConfig.whole_replay_buffer_training=False
TrainerConfig.clear_replay_buffer=False
TrainerConfig.num_iterations=0
TrainerConfig.num_env_steps=10000000
TrainerConfig.num_checkpoints=5
TrainerConfig.evaluate=0
TrainerConfig.debug_summaries=True
TrainerConfig.summary_interval=1000
TrainerConfig.eval_interval=5000
TrainerConfig.num_eval_episodes=20
TrainerConfig.replay_buffer_length=100000
